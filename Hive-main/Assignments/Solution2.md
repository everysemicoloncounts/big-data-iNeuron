## Scenario Based questions:

`1. Will the reducer work or not if you use “Limit 1” in any HiveQL query?`

Yes, the reducer will work even if you use "limit 1" in the hiveql query.

The "limit" clause in a hiveql query limits the number of rows returned by the query. This does not affect the number of mappers or reducers used by the hive runtime.

Reducers are responsible for aggregating the results generated by mappers. It works on the intermediate key-value pairs generated by the mapper, and it doesn't matter if it reduces a single row or a million rows.
So even if you use "limit 1" in your hiveql query, the reducer will still work and produce the final output. However, since the query is limited to one line, the reducer will only have one output.

`2. Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration. Then, what will happen if we have multiple clients trying to access Hive at the same time?`

If you installed Apache Hive on a Hadoop cluster with the default metastore configuration and multiple clients attempt to access Hive simultaneously, those clients will be able to access the metastore simultaneously, but some performance issues may occur.

The default metastore configuration in Hive was a local Derby database, which was not designed to handle concurrent access from multiple clients. When multiple clients attempt to access the metastore simultaneously, they may experience delays or errors due to contention for database resources.

To improve performance and allow concurrent access to the metastore, you can use an external database (such as MySQL or PostgreSQL) as the metastore instead of the default Derby database. External databases can handle multiple client connections and provide better performance and scalability.
In addition to using an external database, you can also use ZooKeeper to configure Hive to use a distributed metastore, which allows multiple instances of Hive to share the same metastore and provides high availability and scalability.

`3. Suppose, I create a table that contains details of all the transactions done by the customers: CREATE TABLE transaction_details (cust_id INT, amount FLOAT, month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;
Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month. But, Hive is taking too much time in processing this query. How will you solve this problem and list the steps that I will be taking in order to do so?`

To solve the problem of slow query processing in Hive for calculating total revenue generated for each month from the "transaction_details" table with 50,000 records, the following steps can be taken:

1. Partition the table: Partitioning the table by the "month" column can significantly speed up the query processing. This can be done using the following command:
```
ALTER TABLE transaction_details ADD PARTITION (month='Jan') LOCATION '/path/to/Jan';

```
The above command creates a partition for the "Jan" month and specifies the location where the data for that partition will be stored.
Repeat this command for all the other months in the table.

2. Use the partitioned column in the query: Modify the query to use the partitioned column in the WHERE clause to filter the data for each month. This will limit the amount of data that needs to be processed by the query.
```
SELECT month, SUM(amount) as total_revenue 
FROM transaction_details 
WHERE month='Jan'
GROUP BY month;

```
Repeat the above query for all the other months in the table.

3. Use bucketing: Bucketing can also help in improving query performance by dividing data into smaller and more manageable chunks. However, in this case, since the table has only 50,000 records, partitioning should be sufficient.

4. Optimize query configuration: If the above steps don't improve the query performance, then you can try optimizing the query configuration parameters such as the number of reducers and the memory allocation for each task.
For example, you can increase the number of reducers to distribute the processing workload across multiple nodes, or increase the memory allocation for each task to avoid out-of-memory errors.

By following the above steps, the query processing time can be significantly reduced, and you can get the total revenue generated for each month from the "transaction_details" table more efficiently.


`4. How can you add a new partition for the month December in the above partitioned table?`

To add a new partition for the month of December in the partitioned table "transaction_details", follow these steps:

1. Create a new directory for the December partition:
```
hdfs dfs -mkdir /path/to/transaction_details/month=Dec

```

2. Move the data files for December to the new directory:
```
hdfs dfs -cp /path/to/transaction_details/datafile_December /path/to/transaction_details/month=Dec

```
Repeat the above command for all the data files for the December partition.

3. Add the partition to the table:
```
ALTER TABLE transaction_details ADD PARTITION (month='Dec') LOCATION '/path/to/transaction_details/month=Dec';

```

The above command will add a new partition for the month of December to the table "transaction_details".

After executing the above steps, the partition for the month of December will be added to the table, and you will be able to run queries on this partition.

`5. I am inserting data into a table based on partitions dynamically. But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column. How will you remove this error?`

The error "FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column" occurs when you try to insert data into a partitioned table dynamically without specifying at least one static partition column. This error can be removed by disabling dynamic partitioning strict mode or by specifying at least one static partition column in the INSERT statement.

To disable dynamic partitioning strict mode, use the following command before inserting the data into the partitioned table:

```
SET hive.exec.dynamic.partition.mode=nonstrict;

```

This command will disable dynamic partitioning strict mode and allow you to insert data into the partitioned table without specifying a static partition column.

Alternatively, you can specify at least one static partition column in the INSERT statement to avoid the error. For example, if your partitioned table has two partition columns "month" and "country", and you want to insert data for the month of January and country "US" dynamically, you can use the following INSERT statement:

```
INSERT INTO TABLE my_table PARTITION (month='Jan', country) VALUES ('value1', 'value2');

```

In the above statement, the "month" partition column is static, and the "country" partition column is dynamic. By specifying a static partition column, you can avoid the error and insert data into the partitioned table dynamically.

`6. Suppose, I have a CSV file – "sample.csv" present in "/temp" directory with the following entries\-
id first_name last_name email gender ip_address
How will you consume this CSV file into the Hive warehouse using built-in SerDe?`

To consume a CSV file "sample.csv" present in the "/temp" directory into the Hive warehouse using built-in SerDe, follow these steps:

1. Create an external table in Hive using the following command:

```
CREATE EXTERNAL TABLE sample_table (
  id INT,
  first_name STRING,
  last_name STRING,
  email STRING,
  gender STRING,
  ip_address STRING
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
  'separatorChar' = ',',
  'quoteChar'     = '\"'
)
LOCATION '/temp';

```
This command will create an external table "sample_table" in Hive, with the same columns as the CSV file, and using the built-in OpenCSVSerde for parsing the CSV data.

2. Load the data from the CSV file into the table using the following command:

```
LOAD DATA INPATH '/temp/sample.csv' INTO TABLE sample_table;

```
This command will load the data from the CSV file into the external table "sample_table". Hive will use the specified SerDe to parse the CSV data and populate the table.

3. Verify the data is loaded correctly using the following command:

```
SELECT * FROM sample_table;

```
This command will display all the records from the external table "sample_table" that was loaded from the CSV file.

By following these steps, you can consume a CSV file using built-in SerDe into the Hive warehouse and query the data using HiveQL.

`7. Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.
So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?`

To solve the problem of creating a single Hive table for lots of small files without degrading the performance of the system, you can use the following approach:

1. Combine the small CSV files using Hadoop's `getmerge` command or any other tool that merges multiple files into a single file. This will create a single large file that can be used to create the Hive table.

2. Create an external table in Hive with the same schema as the CSV files using the following command:

```
CREATE EXTERNAL TABLE my_table (
  id INT,
  name STRING,
  email STRING,
  country STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION '/path/to/combined/file';

```

This command creates an external table "my_table" with the same columns as the CSV files, and uses the built-in TEXTFILE SerDe for parsing the data.

3. Load the data into the Hive table using the following command:

```
LOAD DATA INPATH '/path/to/combined/file' INTO TABLE my_table;

```
This command loads the data from the combined file into the external table "my_table". Hive will use the specified SerDe to parse the data and populate the table.

By using this approach, you can create a single Hive table for lots of small CSV files without degrading the performance of the system. Merging small files reduces the number of files that Hadoop has to deal with, which can improve performance. Additionally, using an external table in Hive allows you to keep the data in HDFS, while still being able to query it using HiveQL.

`8. LOAD DATA LOCAL INPATH ‘Home/country/state/’
OVERWRITE INTO TABLE address;
The following statement failed to execute. What can be the cause?`

The `LOAD DATA LOCAL INPATH` command failed to execute because the path specified in the command is not an absolute path, and is missing the leading forward slash ("/").

Assuming that the "Home" directory is the root directory ("/") of the file system, the correct path to use in the command should be:

```
LOAD DATA LOCAL INPATH '/Home/country/state/'
OVERWRITE INTO TABLE address;

```

The forward slash at the beginning of the path indicates that it is an absolute path, and specifies the root directory of the file system. Without the leading slash, Hive will look for the specified path relative to its current working directory, which may not be what was intended.

`9. Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?`

Yes, it is possible to add 100 nodes to a Hive cluster that already has 100 nodes. This can be done by adding new nodes to the Hadoop cluster and configuring Hive to use the new nodes.

The process of adding new nodes to a Hadoop cluster typically involves the following steps:

1. Install Hadoop and configure the new nodes to join the existing Hadoop cluster.
2. Configure the new nodes to use the same version of Hadoop and have the same configuration settings as the existing nodes in the cluster.
3. Update the Hadoop configuration files on the new nodes to include the same settings as the existing nodes. This includes settings for HDFS, YARN, and MapReduce.
4. Start the Hadoop services on the new nodes.
5. Verify that the new nodes are properly configured and can communicate with the existing nodes in the cluster.

Once the new nodes are added to the Hadoop cluster, Hive can be configured to use the new nodes by updating the configuration settings for Hive. This typically involves updating the `hive-site.xml` file to include the new nodes in the list of available nodes.

Once the configuration is updated, Hive will be able to use the new nodes in its processing. This can help to increase the processing capacity of the Hive cluster and improve performance for large-scale data processing.


## Hive Practical questions:

`1. Hive Join operations

Create a  table named CUSTOMERS(ID | NAME | AGE | ADDRESS   | SALARY)
Create a Second  table ORDER(OID | DATE | CUSTOMER_ID | AMOUNT
)

Now perform different joins operations on top of these tables
(Inner JOIN, LEFT OUTER JOIN ,RIGHT OUTER JOIN ,FULL OUTER JOIN)`

`2. BUILD A DATA PIPELINE WITH HIVE

Download a data from the given location - 
https://archive.ics.uci.edu/ml/machine-learning-databases/00360/

1. Create a hive table as per given schema in your dataset 
2. try to place a data into table location
3. Perform a select operation . 
4. Fetch the result of the select operation in your local as a csv file . 
5. Perform group by operation . 
7. Perform filter operation at least 5 kinds of filter examples . 
8. show and example of regex operation
9. alter table operation 
10. drop table operation
12. order by operation . 
13. where clause operations you have to perform . 
14. sorting operation you have to perform . 
15. distinct operation you have to perform . 
16. like an operation you have to perform . 
17. union operation you have to perform . 
18. table view operation you have to perform . `